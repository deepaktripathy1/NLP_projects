# -*- coding: utf-8 -*-
"""multilabel_classification_nli.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X_0Yl_Y1-dlgJWRv3lA-mi8v0nvzZnjD
"""

!pip install sentence-transformers

from torch.utils.data import DataLoader
import math
from sentence_transformers import util
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator
from sentence_transformers import InputExample
from datetime import datetime
import os
import shutil
import gzip
import csv
import pandas as pd
from zipfile import ZipFile

nli_dataset_path = 'datasets/AllNLI.tsv.gz'

if not os.path.exists(nli_dataset_path):
  util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)

with gzip.open(nli_dataset_path,'rt',encoding='utf8') as fin:
  with open('allnli.tsv','wt') as f_out:
    shutil.copyfileobj(fin,f_out)

# Open file as Dataframe

df = pd.read_csv(r'/content/allnli.tsv',sep='\t',encoding='utf8',on_bad_lines='skip')
df.head()

df_train = df[df['split']=='train']
df_dev = df[df['split']=='dev']
df_test = df[df['split']=='test']

len(df_train),len(df_dev),len(df_test)

# reset index
df_dev.reset_index(drop=True,inplace=True)
df_test.reset_index(drop=True,inplace=True)

df_dev.head(3)

label2int = {"contradiction":0,"entailment":1,"neutral":2}

# Train samples
train_samples = []
df_train['sentence1'] = df_train['sentence1'].astype('str')
df_train['sentence2'] = df_train['sentence2'].astype('str')
for i in range(len(df_train[:100000])):
  label_id = df_train.loc[i,'label']
  train_samples.append(InputExample(texts=[df_train.loc[i,'sentence1'],df_train.loc[i,'sentence2']],
                                    label = label2int[label_id]))
  
# Dev samples
dev_samples = []
df_dev['sentence1'] = df_dev['sentence1'].astype('str')
df_dev['sentence2'] = df_dev['sentence2'].astype('str')
for i in range(len(df_dev)):
  label_id = df_dev.loc[i,'label']
  dev_samples.append(InputExample(texts=[df_dev.loc[i,'sentence1'],df_dev.loc[i,'sentence2']],
                                    label = label2int[label_id]))
  
# Test samples
test_samples = []
df_test['sentence1'] = df_test['sentence1'].astype('str')
df_test['sentence2'] = df_test['sentence2'].astype('str')
for i in range(len(df_test)):
  label_id = df_test.loc[i,'label']
  test_samples.append(InputExample(texts=[df_test.loc[i,'sentence1'],df_test.loc[i,'sentence2']],
                                    label = label2int[label_id]))



label2int = {"contradiction":0,"entailment":1,"neutral":2}

train_samples = []
dev_samples = []

with gzip.open(nli_dataset_path,'rt',encoding='utf8') as f:
  reader = csv.DictReader(f,delimiter='\t',quoting=csv.QUOTE_NONE)
  for row in reader:
    label_id = label2int[row['label']]
    if row['split'] == 'train':
      train_samples.append(InputExample(texts=[row['sentence1'],row['sentence2']],
                                        label=label_id))
    else:
      dev_samples.append(InputExample(texts=[row['sentence1'],row['sentence2']],
                                      label=label_id))

train_batch_size =16
num_epochs = 4
model_save_path = 'output/training_allnli-'+datetime.now().strftime("%Y-%m-%d")

# Define Cross Encoder model
model = CrossEncoder('distilroberta-base',num_labels=len(label2int))

# We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader
train_dataloader = DataLoader(train_samples,
                              shuffle=True,
                              batch_size=train_batch_size)

#During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.
evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples,
                                                           name='AllNLI-dev')

warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up

# Train the model
model.fit(train_dataloader=train_dataloader,
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=10000,
          warmup_steps=warmup_steps,
          output_path=model_save_path)

##### Load model and eval on test set
model = CrossEncoder(model_save_path)

evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(test_samples,name='Quora-test')
evaluator(model)

test = []
for i in range(len(df_test)):
  test.append([df_test.loc[i,'sentence1'],df_test.loc[i,'sentence2']])

predictions = model.predict(test,show_progress_bar=True)
predictions

import numpy as np
pred = np.argmax(predictions,axis=1)
pred

pred = pred.tolist()

df_test['pred_label'] = pred
df_test.head()

y = [label2int[l] for l in df_test['label'].values]
y[:5]

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
print('Accuracy Score:',accuracy_score(y,pred))
print()
print('Classification Report')
print(classification_report(y,pred))

# Confusion Matrix
import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y,pred,normalize='true')
labels = list(set(df_test['label'].values))
df_cm = pd.DataFrame(cm,index=labels,columns=labels)
fig = plt.figure(figsize = (5,5))
ax = sns.heatmap(df_cm,annot=True,fmt='.2f')
ax.set_xlabel('Predicted',labelpad=12)
ax.set_ylabel('Actual',labelpad=12)
ax.set_title('Confusion Matrix')
plt.show()

