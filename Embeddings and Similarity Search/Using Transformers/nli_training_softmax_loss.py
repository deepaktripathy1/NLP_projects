# -*- coding: utf-8 -*-
"""nli_training_softmax_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZMGVC6GwE48hzXdNp_6-JL1pbsy4DFd
"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util
from sentence_transformers import models, losses
from sentence_transformers import LoggingHandler, InputExample
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from torch.utils.data import DataLoader
import math
import logging
from datetime import datetime
import os
import gzip
import csv

#Check if dataset exsist. If not, download and extract  it
nli_dataset_path = 'data/AllNLI.tsv.gz'
sts_dataset_path = 'data/stsbenchmark.tsv.gz'

if not os.path.exists(nli_dataset_path):
  util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)

if not os.path.exists(sts_dataset_path):
  util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)

label2int = {'contradiction':0,"entailment":1,"neutral":2}

train_samples=[]
with gzip.open(nli_dataset_path,'rt',encoding='utf8') as f:
  reader = csv.DictReader(f,delimiter='\t',quoting=csv.QUOTE_NONE)
  for row in reader:
    if row['split'] == 'train':
      label_id = label2int[row['label']]
      train_samples.append(InputExample(texts=[row['sentence1'],row['sentence2']],label=label_id))

len(train_samples)

model_name = "bert-base-uncased"

train_batch_size=16
num_epochs = 1

model_path = 'output/training_nli'

# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings
word_embedding_model = models.Transformer(model_name)

# Apply mean pooling to get one fixed sized sentence vector
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),
                               pooling_mode_mean_tokens=True,
                               pooling_mode_cls_token=False,
                               pooling_mode_max_tokens=False)

model = SentenceTransformer(modules=[word_embedding_model,pooling_model])

# Read the AllNLI.tsv.gz file and create the training dataset
label2int = {'contradiction':0,"entailment":1,"neutral":2}

train_samples=[]
with gzip.open(nli_dataset_path,'rt',encoding='utf8') as f:
  reader = csv.DictReader(f,delimiter='\t',quoting=csv.QUOTE_NONE)
  for row in reader:
    if row['split'] == 'train':
      label_id = label2int[row['label']]
      train_samples.append(InputExample(texts=[row['sentence1'],row['sentence2']],label=label_id))

train_dataloader = DataLoader(train_samples[:50000],shuffle=True,batch_size=train_batch_size)
train_loss = losses.SoftmaxLoss(model=model,
                                sentence_embedding_dimension=model.get_sentence_embedding_dimension(),
                                num_labels=len(label2int))

#Read STSbenchmark dataset and use it as development set

dev_samples = []
with gzip.open(sts_dataset_path,'rt',encoding='utf8') as f:
  reader = csv.DictReader(f,delimiter='\t',quoting=csv.QUOTE_NONE)
  for row in reader:
    if row['split'] == 'dev':
      score = float(row['score'])/5.0
      dev_samples.append(InputExample(texts=[row['sentence1'],row['sentence2']],label=score))

dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples,batch_size=train_batch_size,name='sts-dev')

# Configure the training
warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)

# Train the model
model.fit(train_objectives=[(train_dataloader,train_loss)],
          evaluator=dev_evaluator,
          epochs = num_epochs,
          evaluation_steps = 1000,
          warmup_steps = warmup_steps,
          output_path = model_path)

# Load the stored model and evaluate its performance on test set
test_samples = []
with gzip.open(sts_dataset_path,'rt',encoding='utf8') as f:
  reader = csv.DictReader(f,delimiter='\t',quoting=csv.QUOTE_NONE)
  for row in reader:
    if row['split'] == 'test':
      score = float(row['score']) / 5.0
      test_samples.append(InputExample(texts = [row['sentence1'],row['sentence2']],label=score))

model = SentenceTransformer(model_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples,batch_size=train_batch_size,name='sts-test')
test_evaluator(model,output_path = model_path)

