# -*- coding: utf-8 -*-
"""fast_clustering_sen_tran.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cwL0cc5k-PvPVqyhwgCPgtHbuYMGsOj8

'''
This is a more complex example on performing clustering on large scale dataset.
This examples find in a large set of sentences local communities, i.e., groups of sentences that are highly
similar. You can freely configure the threshold what is considered as similar. A high threshold will
only find extremely similar sentences, a lower threshold will find more sentence that are less similar.
A second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.
The method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).
In this example, we download a large set of questions from Quora and then find similar questions in this set.
'''
"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util
import os
import csv
import time
import pandas as pd

model = SentenceTransformer('all-MiniLM-L6-v2')

# Get all unique sentences from the file

corpus_sentences = set()

with open("./drive/MyDrive/quora_duplicate_questions.tsv",encoding="utf8") as f:
  reader = csv.DictReader(f,delimiter="\t",quoting=csv.QUOTE_MINIMAL)
  for row in reader:
    corpus_sentences.add(row['question1'])
    corpus_sentences.add(row['question2'])
    if len(corpus_sentences) >= 50000:
      break

corpus_sentences = list(corpus_sentences)
corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)

print("Start clustering")
start_time = time.time()

#Two parameters to tune:
#min_cluster_size: Only consider cluster that have at least 25 elements
#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar
clusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)

print("Clustering done after {:.2f} sec".format(time.time() - start_time))

len(clusters)

#Print for all clusters the top 3 and bottom 3 elements
for i, cluster in enumerate(clusters):
    print("\nCluster {}, #{} Elements ".format(i+1, len(cluster)))
    for sentence_id in cluster[0:3]:
        print("\t", corpus_sentences[sentence_id])
    print("\t", "...")
    for sentence_id in cluster[-3:]:
        print("\t", corpus_sentences[sentence_id])

