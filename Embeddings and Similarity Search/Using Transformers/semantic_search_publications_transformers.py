# -*- coding: utf-8 -*-
"""semantic_search_publications_transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/182xWi6__Y-9AKniBx-RF2bh93ONSUGFU
"""

!pip install sentence-transformers

import json
import os
from sentence_transformers import SentenceTransformer, util

# Load the papers dataset
file = 'emnlp2016-2018.json'

if not os.path.exists(file):
  util.http_get('https://sbert.net/datasets/emnlp2016-2018.json',file)

with open(file) as f:
  papers = json.load(f)

print(len(papers), "papers loaded")

with open('./emnlp2016-2018.json','r') as f:
  d = json.load(f)

import pandas as pd
df = pd.DataFrame(d)
df.head()

# Load the model

model = SentenceTransformer('allenai-specter')

# To encode the papers, we must combine the title and the abstracts 
# to a single string

paper_texts = [paper['title'] + '[SEP]' + paper['abstract'] for paper in papers]
paper_texts[:3]

# Encode the papers
embeddings = model.encode(paper_texts,convert_to_tensor=True)
print("Shape of embeddings:",embeddings.shape)

# Define a function to search for similar papers given title and abtract

def search_papers(title,abstract):
  query_embedding = model.encode(title+'[SEP]'+abstract,convert_to_tensor=True)

  search_hits = util.semantic_search(query_embedding,embeddings)
  search_hits = search_hits[0]

  print("Paper:",title)
  print("Most similar papers:")
  for hit in search_hits:
    related_paper = papers[hit['corpus_id']]
    print('{:.2f}\t{}\t{} {}'.format(hit['score'],related_paper['title'],
                                     related_paper['venue'],
                                     related_paper['year']))

# This paper was the EMNLP 2019 Best Paper
search_papers(title='Specializing Word Embeddings (for Parsing) by Information Bottleneck', 
              abstract='Pre-trained word embeddings like ELMo and BERT contain\
              rich syntactic and semantic information, resulting in\
              state-of-the-art performance on various tasks. We propose a very\
              fast variational information bottleneck (VIB) method to\
              nonlinearly compress these embeddings, keeping only the\
              information that helps a discriminative parser. We compress each\
              word embedding to either a discrete tag or a continuous vector.\
              In the discrete version, our automatically compressed tags form\
              an alternative tag set: we show experimentally that our tags\
              capture most of the information in traditional POS tag\
              annotations, but our tag sequences can be parsed more accurately\
              at the same level of tag granularity. In the continuous version,\
              we show experimentally that moderately compressing the word\
              embeddings by our method yields a more accurate parser in 8 of 9\
              languages, unlike simple dimensionality reduction.')

# EMNLP 2020 paper on making Sentence-BERT multilingual
search_papers(title='Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation',
              abstract='We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.')

